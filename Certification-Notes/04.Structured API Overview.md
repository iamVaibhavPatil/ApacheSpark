## Introduction
Apache Spark 2 has introduced new Structured APIs - DataFrames, SQL, and DataSets

The Structured APIs are the tool for manipulating all sorts of data, from unstructured log files to semi-structured CSV files and highly structured Parquet files.

These structured APIs apply to both `batch` and `streaming computations`. So, when we work with Structured API, it is simple to migrate from batch to streaming with little to no efforts.

The Structured APIs are the fundamental abstractions that we will use to write the majority of data flows.

Spark is a distributed programming model in which the user specifies the `transformations`. Multiple transformations build up a `directed acyclic graph` of instructions. An `action` begins the process of executing that graph of instructions, as a single job, by breaking it down into stages and tasks to execute across the cluster. The logical structures that we manipulate with transformations and actions are DataFrames and DataSets. To create a new DataFrame or DataSet, we call a transformation. To start computation or convert to native language types, we call an `action`.

## DataFrames and DataSets
DataFrames and DataSets are distributed table-like collections with well-defined rows and columns. DataFrames and DataSets represent immutable, lazily evaluated plans that specify what operations to apply to data residing at a location to generate some output.

When we perform an action on a DataFrame, we instruct Spark to perform the actual transformations and return the results.

`Tables and views` are basically the same things as DataFrames. We just execute SQL against them instead of DataFrame code.

### Schemas  
A schema defines the column names and types of a DataFrame. We can define schemas manually or read a schema from a data source often called as `schema on read (inferSchema)`.

## Structured Spark Types  
Spark is effectively a programming language of it's own. Internally, Spark uses an engine called `Catalyst` that maintains it's own type information through the planning and processing of work.

Spark types map directly to the different language APIs that Spark maintains and there exists a `lookup table` for each of these in Scala, Java, Python, SQL and R.

Even if we use Spark's Stuctured APIs from Python or R. The majority of manipulations will operate strictly on `Spark Types, not Python types`.

For example- Following code does not perform addition in Scala or Python; it actually  performs addition  purely in Spark:

// in Scala
```
val df = spark.range(500).toDF("number")
df.select(df.col("number") + 10)
```

// in Python
```
df = spark.range(500).toDF("number")
df.select(df["number"] + 10)
```

Spark will convert an expression written in an input language to Spark's internal Catalyst representation of that same type information. It will then operate on that internal representation.

### DataFrames Versus Datasets
Within Structured APIs, there are two more APIs-   

**untyped called as DataFrames** - DataFrames have types but Spark maintains them completely and only checks whether those types line up to those specified in the schema at runtime.

**typed called as DataSets**- DataSets checks whether types conform to the specification at `compile time`. DataSets are only available to Java Virtual Machine(JVM)- Based languages(Scala and Java). We specify types with `case classes or Java Beans`.

To Spark(in Scala), DataFrames are simply Datasets of type `Row`. Row type is Spark's internal representation of it's optimized im-memory format for computation. This format makes for highly specialized and efficient computation because rather than using JVM types, which can cause high `garbage-collection` and `object instantiation` costs.

To Spark(in Python or R), there is not such thing as a Datasets; everything is a DataFrame and therefor we always operate on that optimized format.

### Columns
Columns represent a `simple type` like an integer or string, a `complex type` like an array or map, or a null value. Spark internally tracks all of this information and offers variety of ways, with which can transform columns.

### Rows
A row is a record of data. Each record in a DataFrame must of type `Row`. We can create these rows manually from SQL, from Resilient Ditributed Datasets(RDDs), from data sources or manually from scratch.

// in Scala
```
val df = spark.range(2).toDF().collect()
>>> df: Array[org.apache.spark.sql.Row] = Array([0], [1])
```

// in Python
```
spark.range(2).collect()
```

### Spark Types
Spark has large number of internal types representations.

#### Scala
To work with Scala types, use following:

```
import org.apache.spark.sql.types._
val b = ByteType
```

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/scala_types.PNG?raw=true "Scala Language Types")

#### Java
To work with Java types, we should use Factory methods in the following package:

```
import org.apache.spark.sql.types.DataTypes;
ByteType x = DataTypes.ByteType
```

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/java_types.PNG?raw=true "Java Language Types")

#### Python
To work with Python types, use following:

```
from pyspark.sql.types import *
b = ByteType()
```

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/python_types.PNG?raw=true "Python Language Types")




## Overview of Structured API Execution  
Spark internally has many steps and optimizations involved before the code gets executed on cluster. Let's walk through the excution of a single structured API query from user code to executed code. This will help us understand the process of writing and executing code on clusters.

Here's an overview of the steps:

1. Write a DataFrame/DataSet/SQL Code  
2. If Valid code, Spark converts this to a `Logical Plan`.  
3. Spark transform this `Logical Plan` to a `Physical Plan`, checking for optimizations along the way.  
4. Spark then executes this `Physical Plan(RDD manipulations)` on the cluster.  

When we submit the code to Spark either through the console or via a submitted job. This code then passes through the `Catalyst Optimizer`, which decides how the code should be executed and lays out a plan for doing so. Finally, the code is run and the result is returned to the user.

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/Catalyst_Optimizer.PNG?raw=true "Catalyst Optimizer")


### Logical Planning

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/structured_api_logical_planning_process.PNG?raw=true "The  Structured API Logical Planning Process")

The first phase of execution is meant to take user code and convert it into a logical plan. The logical plan only represents a set of abstract transformations that do not refer to executors or drivers, it's purely to convert the user's set of expressions into the most optimized version.

It does this by converting user code into an `unresolved logical plan`. This plan is unresolved because although code is valid, the tables or columns that it refers to might or might not exist.

Spark uses the `catalog`, a repository of all table and DataFrame information, to resolve columns and tables in `analyzer` . The analyzer might reject the unresolved logical plan if the required table or column name does not exist in the catalog. If the analyzer can resolve it, the result is passed through the `Catalyst Optimizer`, a collection of rules that attempt to opimize the logical plan by pushing down predicates or selections.

Packages can extend the Catalyst to include their own rules for domain-specific optimizations.

### Physical Planning

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/structured_api_physical_planning_process.PNG?raw=true "The  Physical Planning Process")

After creating an optimized logical plan, Spark then begins the physical planning process. The `physical plan`, often called a `Spark plan`, specifies how the logical plan will execute on the cluster by generating different physical execution strategies and comparing them through a cost model.

An example of cost comparision might be choosing how to perform a given join by looking at the physical atrributes of a given table like how big the table is or how big it's partitions are.

Physical planning results in a series of RDDs and transformations. This result in why you might have heared Spark referred to as a compiler- it takes queries in DataFrames, Datasets, and SQL and compiles them into RDD transformations for you.

### Exceution
Upon selecting a physical plan, Spark runs all of this code over RDDs, the lower-level programming interface of Spark.

Spark performs further optimizations at runtime, generating native Java bytecode that can remove entire tasks or stages during execution. Finally result is returned to the user.

## Internal Architecture of Catalyst Optimizer

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/Catalyst_Optimizer_end_to_end.PNG?raw=true "Cata Optimizer")

https://www.youtube.com/watch?v=5ajs8EIPWGI

https://www.youtube.com/watch?v=GDeePbbCz2g

https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html