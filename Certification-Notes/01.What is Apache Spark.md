## Introduction
Apache Spark is a unified computing engine and set of libraries for parallel data processing on computer clusters. 

1. Spark support multiple widely used programming languages like **Python, Java, Scala, R**.  
2. Spark includes support for libraries for diverse tasks ranging from SQL to streaming and machine learning.  
3. Spark can run on single computer or cluster of computers which makes it easy system to start with and scale up the big data processing at large scale.  
4. Spark is composed of a number of different components as below

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/SparkToolKit.PNG?raw=true "Spark Toolkit")

**Unfied**  
Spark is unified platform which means it is designed to support a wide range of data analytics tasks, ranging from simple data loading and SQL queries to machine learning and streaming computations, over the same computing engine and with a consistent set of APIs.

The main insight behind this goal is that real-world data analytics tasks- whether they are interactive analytics in tool such as Jupyter notebook or traditional production applications which tends to combine many differnt processing types and libraries.

Spark's unified nature makes these tasks both easier and more efficient to write. Spark provides consistent, composable APIs that you can use to build an application out of smaller pieces or out of existing libraries. It also makes it easy for us to write our own analytics libraries on top.

Spark's APIs are also designed to enable high performance by optimizing across different libraries and functions composed together in a user program.

For Example- If we load data using SQL query and then evaluate machine learning model over it using Spark's ML library, then engine can combine these steps into one scan over the data. The combination of general APIs and high performance execution, no matter how we combine them makes Sparks a power platform for interactive and production applications.

Data scientists benefits from unified set of libraries e.g. Python and R when doing modeling and web developers benefits from unified frameworks such as NodeJS or Django. Before Spark, no open source systems have tried to provide this type of unified engine for parallel data processing, mean that we had to stitch together an application out of multiple APIs and systems.

**Computing Engine**  
Spark strives for unification, but it carefully limits its scope to a computing engine. By this, we mean that Spark handles loading data from storage systems and performing a computation on it, but it's not permanent storage itself.

We can use Spark with a wide variety of persistent storage systems, 

- Azure Storage  
- Amazon S3  
- Distributed file systems such as Apache Hadoop  
- Key-value stores such as Apache Cassandra  
- Message buses such as Apache Kafka  

Key motivation here is that most of the data already resides in a mix of storage systems. Data is expensive to move so Spark focus on performing the computations over the data, no matter where it resides.

Spark's focus on computation makes it different from the earlier big data software platforms such as Apache Hadoop. Hadoop included both storage systems(the Hadoop file system, designed for low cost storage over clusters of commodity servers) and computing system(MapReduce), which were closely integrated together. However, this choice makes it difficult to run one of the systems without the other. More important, this choice also makes it challenge to writing applications that access data stored anywhere else.

Spark runs well on Hadoop storage, but it is used broadly in envrionments for which Hadoop architecture does not make sense, such as the public cloud or streaming applications.

**Libraries**  
Sparks supports standard libaries that ship with the engine as well as a wide array of external libraries published as third party packages by open source communities. Sparks includes libraries for SQL and structured data(Spark SQL), machine learning(MLlib), stream processing(Spark Streaming and new Structure Streaming), and graph analytics(GraphX).

There are other open source libraries from connectors for various storage systems to machine learning algorithms. We can check all the spark supported libraries- https://spark-packages.org/


## Big Data Problem  
The need for Apache Spark arises due to changes in economic factors that underline computer applications and hardware.

Computer became faster every year through processor speed increase. The new processor each year could run more instructions per seconds than the previuos year's. As a result applications also automatically became faster every year, without any changes needed to their code. This trend lead to a large and estalished ecosystem of applications building up over the time, most of which were designed to run only on single processor.

Unfortunately, this trend in hardware stopped around 2005, due to hard limits in heat dissipation, hardware developer stopped making individual processor faster, and switched toward adding more parallel CPU cores all running at the same speed. This change meant that suddenly applications needed to be modified to add parallelism in order to run faster, which set the stage for new programming models such as Apache Spark.

On the top of that technologies for storing and collecting data did not slow down, when processor speeds did. The cost to store 1 TB of data continues to drop by roughly two times every 14 months, meaning that it it very inexpensive for organizations of all sizes to store the large amount of data. Moreover, many  of the technologies for collecting data(sensors, cameras, public datasets etc) continue to drop in cost and improve in resolution.

The end result is a world in which collecting data is extermely inexpensive, but processing it requires a large, parallel computations often on clusters of machines. Moreover, in this new world, the software developed in the past 50 years cannot automatically scale up, and neither can the traditional programming models for data processing apllications, creating the need for new programming models. It is this world that Apache Spark was built for.

Apache Spark began at UC Berkeley in 2009 as the Spark research project.

## Install Apache Spark  
We can install Apache Spark locally or we can use databricks online collaborative notebook -

Below Steps can be used to download and install Spark locally
http://sundog-education.com/spark-scala/



