## Spark's Basic Architecture
A single computer or laptop works perfectly well for watching movies or working with spreadsheet software or working on small datasets, but there are some things that single computer is not powerful enough to perform. 

One particularly challenging area is data processing. Single machines do not have enough power and resources to perform computation on huge amount of information. A cluster or group of computers, pools the resources of many machines together, giving us the ability to use all the cumulative resources as if they were single computer. Now, a group of machines alone is not powerful, you need a framework to coordinate work across them. Spark does just that, managing and coordinating the execution of task on data across a cluster of computers.

The cluster of machines that Spark will use to execute tasks is managed by a cluster manager. Spark support many cluster managers -

- Spark Standalone cluster manager  
- YARN  
- MESOS  

We then submit Spark applications to these cluster manager, which will grant resources to our application so that we can complete our work.

### Spark Applications  
Spark Applications consist of **a driver process** and **a set of executor processes**. 

#### Driver Process  
The driver process which sits on a node in cluster and runs the main() function is responsible for 3 things-

- Maintaining information about the Spark Applications  
- Responding to a user's program or input  
- Analyzing, distributing, and scheduling work across the executors.

The driver process is absolutely essential. It is the heart of Spark applications and maintains all the relevant information during the lifetime of the application.

#### Executor Processes
The executors are responsible for actually carrying out the work that the driver assigns them. Executors are responsible for only 2 things- 

- Executing the code assigned to them by the driver  
- Reporting the state of computation on that executor back to the driver node.  

Below diagram demonstrate how the cluster manager controls the physical machines and allocates resources to Spark Applications. There could be multiple Spark applications running on a cluster at the same time.

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/Spark_Basic_Architecture.PNG?raw=true "Spark Basic Architecture")

Spark, in addition to it's cluster mode, also has a **local mode**. The driver and executors are simply processes, which means that they can live on the same machine or different machines. In local mode, the driver and executors run(as thread) on single computer instead of a cluster.

Key points about Spark Applications-  

- Spark employs a cluster manager that keeps track of resources available  
- The driver process is responsible for executing the driver program's commands across the executors to complete given task.

Executors, for the most part will always be running Spark code. However, the driver can be driven from number of different language through Spark language APIs.

## Spark Language APIs
Spark supports Scala, Java, Python, SQL, R. Spark presents core concepts in every lanaguge; these concepts are them translated into Spark code that runs on the cluster of machines. If we just used Structured APIs, we can expect all languages to have similar performance characteristics.

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/SparkSessionAndSparkLanguageRel.PNG?raw=true "Relationship Between Spark Session and Spark Language")

SparkSession obejct is available to user, which is entrance point to running Spark code. When using Python or R, we dont need to write explicit JVM instructions: instead, Spark translates Python code into a code which can run on executors JVM's.

Spark have 2 fundamental set's of API -

- Low Level unstructured APIs
- Higer level structured APIs

## SparkSession
When we write the Spark Applications, we send the user commands and data to Application using SparkSession object.

### Spark using interactive console
We can start the Spark interactive console application using  `./bin/spark-shell` to acess the scala console OR `./bin/pyspark` to access Python console.

When we start the console, we get built in SparkSession object called as `spark`. You can check by typing in console

Scala Console
```
scala> spark
res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@2cea921a
```

Pyton Console
```
>>> spark
<pyspark.sql.session.SparkSession object at 0x000002009708AB38>
>>>
```

### Spark using Stand alone application
We can create standalone applications and submit them to Spark using `spark-submit` command line. When we create standalone application, we must create SparkSession object to send the commands and data to Spark.

## DataFrames
A DataFrame is the most common Structured API and simply represents a table of data with rows and columns. The list that defines the columns and the types within those columns is called the `schema`.

We can think of DataFrame as a spreadsheet with named columns. Fundamental difference is: a spreadsheet sits on one computer in one specific location, whereas a Spark DataFrame can span thousands of computers.

The reason for putting the data on more than one computer should be intuitive:either the data is too large to fit on one machine or it would simply take too long to perform that computation on one machine.

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/SparkDistributedVsSingle.PNG?raw=true "Relationship Between Spark Session and Spark Language")

R and Python also have DataFrame concepts and they exists on one machine rather than multiple machines. Spark has language interfaces for both Python and R, it's quite easy to convert Pandas(Python) DataFrames to Spark DataFrames, and R DataFrames to Spark DataFrames.

Spark has several core abstractions: `DataSets, DataFrames, SQL Tables, and Resilient Distributed DataSets(RDDs)`. These different abstractions all represent distributed collections of data.

#### Partitions
To allow every executor to perform work in parallel, Spark breaks up the data into chunks called `Partitions`. A partition is a collection of rows that sit on one physical machine in cluster.

A DataFrame's partition represent how the data is physically distributed across the clusters of machines during execution. 

If we have one partition, Spark will have a parallelism of only one, even if we have thousands of executors. If we have many partitions but only one executor, Spark will still have a parallelism of only one because there is only one computation resource.

With DataFrames we don't need to manipulate partitions manually or individually. We simply need to specify high-level transformations of data in physical partitions, and Spark determines how this work will actully execute on clsuter.

Let's create a DataFrame with one column containing 1000 rows with values from 0 to 999. This range of numbers represents a `distributed collection`. When run on a cluster, each part of this range of numbers exists on a different exectuor.

// In Scala
```
val myRange = spark.range(1000).toDF("number")
```
myRange: org.apache.spark.sql.DataFrame = [number: bigint]


// In Python
```
myRange = spark.range(1000).toDF("number")
```

## Transformations
In Spark, the core data structures are immutable, meaning they cannot be changed after they are created. To change a DataFrame, we need to instruct Spark, how we like to modify it, these instructions are called as transformations.

For Example- A simple transformation to find all even numbers in our current DataFrame:

//In Scala
```
scala> val divisBy2 = myRange.where("number % 2 = 0")
divisBy2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [number: bigint]
```

//In Python
```
divisBy2 = myRange.where("number % 2 = 0")
```

Spark will not act on transformations untill we call an action. Transformations are the core of how we express business logic using Spark. There are 2 types of transformations - 

### Narrow Tranformations  
Transformations consisting of narrow dependencies are those for which each input partition will contribute to only one output partition.

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/Narrow_Transformations.PNG?raw=true "Narrow Transformations")

In privious code, the `where` statement specifies a narrow dependency, where only one partition contributes to at most one output partition.

With narrow tranformations, Spark will automatically perform an operation called `pipelining`, meaning that if specify multiple filters on DataFrames, they will all be performed `in-memory`.

### Wide Transformations  
A wide dependency style transformation will have input partitions contributing to many output partitions. It is also called as `Shuffle` whereby Spark will exchange partitions across the cluster.

When we perform a shuffle, Spark writes the results to `disk`.

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/Wide_Transformations.PNG?raw=true "Narrow Transformations")

### Lazy Evaluation
Lazy evaluation means that Spark will wait unitl the very last moment to execute the graph of computation instructions. In Spark, instead of modifying the data immediately when we express some operation, Spark build up a plan of transformation that Spark would like to apply to source data.

By waiting untill the last minute to execute the code, Spark compiles this plan from raw DataFrame transformations to a streamlined physical plan that will run as efficiently as possible across cluster. This provides immense benefits, because Spark can optimize entire data flow from end to end.

An example if this is something call `predicate pushdown` on DataFrames. If we build a large Spark job but specify a filter at the end that only requires us to fetch one row from our source data, the most efficient way to execute this is to access the single record that we need. Spark will optimize this for us by pushing the filter down automatically.

## Actions
Transformations allow us to build up our logical transformation plan. To trigger the computation, we run `action`. An action instructs Spark to compute a result from a series of transformations.

The simplest action is `count`, which gives us total number of records in the DataFrame:

```
scala> divisBy2.count()
res0: Long = 500
```

There are 3 kinds of actions:

- Action to view data in the console
- Action to collect data to native object in the respective language
- Action to write to output data sources

When we ran count() action, we started a Spark job that runs our `filter tranformation(a narrow transformation)`, then `an aggregation(a wide transformation)` that performs the counts on a per partition basis.

We can see this in Spark UI as well on - http://localhost:4040

## An End-to-End Example
We will use Spark to analyze flight data from United States Bureau of Transportation statistics.

CSV files are semi-structured data format, with each row in the file representing a row in our future DataFrame.

Spark includes the ability to read and write from a large number of data sources. To read the file, we will use `DataFrameReader` that is associated with our SparkSession.

We will sepcify the file format as well as many other options. In this case we want to use `schema inference` which means that we want Spark to take best guess at what the schema of our DataFrame should be.

We also want to specify the first row is header in the file. To get the schema information, Spark reads in a little bit of data and then attempts to parse the types in those rows according to the type available in the Spark.

**We have an option of specifying the shcema when we read the file. We should always specify the schema while reading the production application.**

// in Scala
```
val flightData2015 = spark
  .read
  .option("inferSchema", "true")
  .option("header", "true")
  .csv("D:\\Github\\ApacheSpark\\Certification-Notes\\data\\flight-data\\csv\\2015-summary.csv")


flightData2015: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]
```

// in python
```
flightData2015 = spark\
  .read\
  .option("inferSchema", "true")\
  .option("header", "true")\
  .csv("/data/flight-data/csv/2015-summary.csv")
```

Each of these DataFrames have a set of columns with an unspecified number of rows. The reason the number of rows is unspecified is because reading the data is a transformation and therefore a lazy operation. Spark peeked at only a couple of rows of data to try to guess what type of each column should be.

Below diagram provides the illustration of the CSV file being read into a DataFrame and then being converted into a local array or list of rows.

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/CSV_To_Array_Of_Rows.PNG?raw=true "Reading a CSV file into a DataFrame and converting it to a local array or list of rows")

If we perform the take action on the DataFrame, we will get data from CSV file

```
flightData2015.take(3)

res1: Array[org.apache.spark.sql.Row] = Array([United States,Romania,15], [United States,Croatia,1], [United States,Ireland,344])
```

Let's `sort`(transformation) our data according to `count` column, which is integer type. Sort does not modify the DataFrame. We use `sort` as a transformation that returns new DataFrame by transforming the previous DataFrame.

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/reading-sorting-collecting.PNG?raw=true "Reading, Sorting and Collecting a DataFrame")

Nothing happens to the data when we call the sort because it's just a transformation. However, we can see that Spark is building up a plan for how it will execute across the cluster by looking at the `explain` plan. We can call `explain` on any DataFrame object to see the DataFrame's lineage(how Spark will execute this query).

```
scala> flightData2015.sort("count").explain()

== Physical Plan ==
*Sort [count#29 ASC NULLS FIRST], true, 0
+- Exchange rangepartitioning(count#29 ASC NULLS FIRST, 200)
   +- *FileScan csv [DEST_COUNTRY_NAME#27,ORIGIN_COUNTRY_NAME#28,count#29] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/D:/Github/ApacheSpark/Certification-Notes/data/flight-data/csv/2015-summa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>
```

We can read the explain plan from top to bottom, the top being the end result, and the bottom being the source of data. In above explain plan, we can see keywords like `sort, exchange, and FileScan`. That's because the sort of our data is actually a wide transformation because rows will need to be compared with one another.

Explain plans are helpful tools for debugging and improving knowldege of jobs executions.

By default, **when we perform a shuffle, Spark outputs 200 shuffle partitions.** Let's set this value to 5 to reduce the number of the output partitions from the shuffle:

```
spark.conf.set("spark.sql.shuffle.partitions","5")
```

```
scala> flightData2015.sort("count").take(2)

res5: Array[org.apache.spark.sql.Row] = Array([United States,Singapore,1], [Moldova,United States,1])
```

Below diagram shows the process of logical and physical DataFrame manipulation.

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/logical-and-physical.PNG?raw=true "Process of logical and physical DataFrame manipulation")

The logical plan of transformations that we build up defines a lineage for the DataFrame so that at any given point in time, Spark knows how to recompute any partition by performing all of the operations it had before on the same input data. This sits at the heart of Spark's programming model- functional programming where the same inputs always result in the same outputs when the transformations on that data stay constant.

Spark do not manipulate the physical data instead it configures the physical execution characteritics through things like shuffle partitions parameter that we set.

With that, we ended up with five output partitions because that's the values we specified in the shuffle partition. We can change this help control the physical execution characteritics of Spark jobs.

We can check the physical and logical execution characteritics of our jobs in Spark UI.

### DataFrames and SQL
We can express our business logic in `SQL` or `DataFrames`(either in R, Python, Scala or Java) and Spark will compile the logic down to underlying plan(that we can see in explain plan) before actually executing the code.

With Spark SQl, we can register any DataFrame as a `table or view(a temporary table)` and query it using pure SQL. There is no performance difference between writing SQL queries or writing DataFrame code, they both compile to same underlying plan that we specify in DataFrame code.

We can make any DataFrame into table or view -

```
flightData2015.createOrReplaceTempView("flight_data_2015")
```

We can now use `spark.sql` function to query our data in SQL which conveniently returns a new DataFrame, which can be used for perform more transformation on the returned data.

We will get same explain plan for both SQL and DataFrame APIs-

// in scala
```
val sqlWay = spark.sql("""
SELECT DEST_COUNTRY_NAME, count(1)
FROM flight_data_2015
GROUP BY DEST_COUNTRY_NAME
""")

sqlWay: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, count(1): bigint]

val dataFrameWay = flightData2015
  .groupBy('DEST_COUNTRY_NAME)
  .count()

dataFrameWay: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, count: bigint]

sqlWay.explain()
== Physical Plan ==
*HashAggregate(keys=[DEST_COUNTRY_NAME#27], functions=[count(1)])
+- Exchange hashpartitioning(DEST_COUNTRY_NAME#27, 5)
   +- *HashAggregate(keys=[DEST_COUNTRY_NAME#27], functions=[partial_count(1)])
      +- *FileScan csv [DEST_COUNTRY_NAME#27] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/D:/Github/ApacheSpark/Certification-Notes/data/flight-data/csv/2015-summa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>

dataFrameWay.explain
== Physical Plan ==
*HashAggregate(keys=[DEST_COUNTRY_NAME#27], functions=[count(1)])
+- Exchange hashpartitioning(DEST_COUNTRY_NAME#27, 5)
   +- *HashAggregate(keys=[DEST_COUNTRY_NAME#27], functions=[partial_count(1)])
      +- *FileScan csv [DEST_COUNTRY_NAME#27] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/D:/Github/ApacheSpark/Certification-Notes/data/flight-data/csv/2015-summa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>
```

// in Python
```
sqlWay = spark.sql("""
SELECT DEST_COUNTRY_NAME, count(1)
FROM flight_data_2015
GROUP BY DEST_COUNTRY_NAME
""")

dataFrameWay = flightData2015\
  .groupBy("DEST_COUNTRY_NAME")\
  .count()

sqlWay.explain()
dataFrameWay.explain()
```

Notice that these plans compile to exact same underlying plan.

DataFrames and SQL in Spark have a huge number of manipulations available. There are undreads of functions that we can use and import to help resolve the big data problems faster.

**For example-** We can imprt `max` function, to get the maximum number of flights to and from any given location. This scans each relevant column in the DataFrame and checks whether it's greater than the previous values that have been seen. This is a transformation, because we are effectively filtering down to one row.

```
spark.sql("SELECT max(count) from flight_data_2015").take(1)

res10: Array[org.apache.spark.sql.Row] = Array([370002])
```

// in Scala
```
import org.apache.spark.sql.functions.max
flightData2015.select(max("count")).take(1)

res11: Array[org.apache.spark.sql.Row] = Array([370002])
```

// in Python
```
from pyspark.sql.functions import max
flightData2015.select(max("count")).take(1)
```

**More Example-** Let's find the the top 5 destination countries in the data. This is `multi-transformation` query.

We can do this using Spark SQL as well as DataFrames. Lets do using SQL first -

// in Scala
```
val maxSql = spark.sql("""
SELECT DEST_COUNTRY_NAME, sum(count) as destination_total
FROM flight_data_2015
GROUP BY DEST_COUNTRY_NAME
ORDER BY sum(count) DESC
LIMIT 5
""")

maxSql.show()

+-----------------+-----------------+
|DEST_COUNTRY_NAME|destination_total|
+-----------------+-----------------+
|    United States|           411352|
|           Canada|             8399|
|           Mexico|             7140|
|   United Kingdom|             2025|
|            Japan|             1548|
+-----------------+-----------------+
```

// in Python
```
maxSql = spark.sql("""
SELECT DEST_COUNTRY_NAME, sum(count) as destination_total
FROM flight_data_2015
GROUP BY DEST_COUNTRY_NAME
ORDER BY sum(count) DESC
LIMIT 5
""")

maxSql.show()
```

Now, let's do the same thing using DataFrame syntax. DataFrame syntax is semantically similar but slightly different in implementation and ordering. But underlying plans for both of them are same.

// in Scala
```
import org.apache.spark.sql.functions.desc

flightData2015
  .groupBy("DEST_COUNTRY_NAME")
  .sum("count")
  .withColumnRenamed("sum(count)", "destination_total")
  .sort(desc("destination_total"))
  .limit(5)
  .show()

+-----------------+-----------------+
|DEST_COUNTRY_NAME|destination_total|
+-----------------+-----------------+
|    United States|           411352|
|           Canada|             8399|
|           Mexico|             7140|
|   United Kingdom|             2025|
|            Japan|             1548|
+-----------------+-----------------+
```

// in Python
```
from pyspark.sql.functions import desc

flightData2015\
  .groupBy("DEST_COUNTRY_NAME")\
  .sum("count")\
  .withColumnRenamed("sum(count)", "destination_total")\
  .sort(desc("destination_total"))\
  .limit(5)\
  .show()
```

**Compuation Plan**  
Below diagram shows the set of steps that we perform in "code". The true execution pan will differ from the one shown in the diagram because of optimizations in physical execution.

The execution plan is `directed acyclic graph(DAG)` of transformations, each resulting in a new immutable DataFrame, on which we call an action to generate a result.

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/dataframe_transformation_flow.PNG?raw=true "DataFrame transformation flow")

**Steps**  
1. First steps is to read the data. We have defined the DataFrame priviously, but Spark does not actually read it untill an action is called on the DataFrame or one derived from original DataFrame.

2. Second step is grouping, technically when we call `groupBy`, we end up with a `RelationalGroupedDataSet`, which is again a DataFrame that has a grouping specified but needs user to sepcify an aggregation before it can be queries further. We have specified that we are going to be grouping by a key(or set of keys) and that now we are going to perform an aggregation over each one of those keys.

3. Third step is to specify an aggregation. Let's use `sum` aggregation method. This takes a column name as input and result of sum method call is a new `DataFrame`. This is also another transformation, no computation is done yet.

4. Fourth steps is renaming the column. This is just another transformation and not a computation.

5. Fifth step sorts the data such that if we take result of the top of the DataFrame, they would have largest values in destination_total column.

6. Sixth step, we specify the limit and get the number of rows to fetch from the sorted data.

7. In seventh steps we call an action `show` to execute the job and perform various transformation on the data and return the result.


**Explain Plan**  
```
import org.apache.spark.sql.functions.desc

flightData2015
  .groupBy("DEST_COUNTRY_NAME")
  .sum("count")
  .withColumnRenamed("sum(count)", "destination_total")
  .sort(desc("destination_total"))
  .limit(5)
  .explain()

== Physical Plan ==
TakeOrderedAndProject(limit=5, orderBy=[destination_total#133L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#27,destination_total#133L])
+- *HashAggregate(keys=[DEST_COUNTRY_NAME#27], functions=[sum(cast(count#29 as bigint))])
   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#27, 5)
      +- *HashAggregate(keys=[DEST_COUNTRY_NAME#27], functions=[partial_sum(cast(count#29 as bigint))])
         +- *FileScan csv [DEST_COUNTRY_NAME#27,count#29] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/D:/Github/ApacheSpark/Certification-Notes/data/flight-data/csv/2015-summa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>
```

As we can see in the explain plan for the DataFrame, there are 7 steps involved in the execution that take us all the way back to the source of the data.

Altought this explain plan does not match exact "computation plan",  all the pieces are there. We can see the limit statement as well as the orderBy. We can see that aggregation happens in 2 phases, in `partial_sum` calls. This is because summing a list of numbers is commutative, and Spark can perform the sum, partition by partition.

We can write the result to another source as well like database.
