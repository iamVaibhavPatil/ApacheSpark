## Introduction
Spark's core concepts like transformations and actions in the context of Spark's structured APIs are the conceptual building block and foundation of Apache Spark ecosystem and libraries.

Spark is composed of lower level APIs and the Structured APIs, and then series of standard libraries for additional functionality.

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/SparkToolKit.PNG?raw=true "Spark Basic Architecture")

Spark's libraries support a variety of different tasks from graph analysis and machine learning to streaming and integrations with different computing and storage systems.

## Running Production Applications  
We can run the Spark applications using `spark-submit` command line interface. In this, we can package the application using the Spark supported language such as Scala, Java, Python, and R and submit it. Spark application run in local mode or can be submitted to Spark cluster manager like Standalone, Mesos, and YARN.

`spark-submit` provides various options with which we can specify the resources required by our application.

We will run below example that is packaged with Spark which calculates the digits of `pi` to certain level of estimation. Here we are running the application in local mode with main class to run from the jar file.

// Scala Example
```
spark-submit --class org.apache.spark.examples.SparkPi --master local C:\\spark\\examples\\jars\\spark-examples_2.11-2.2.1.jar 10

Pi is roughly 3.1437311437311437
```

// Python example
```
spark-submit --master local C:\\spark\\examples\\src\\main\\python\\pi.py 10

Pi is roughly 3.1437311437311437
```

By changing the `master` argument of spark-submit, we can submit the same application to cluster running Sparks's standalone cluster manager, Mesos or YARN.

## Datasets: Type-Safe Structured APIs