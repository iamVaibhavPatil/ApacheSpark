## Introduction
Spark's core concepts like transformations and actions in the context of Spark's structured APIs are the conceptual building block and foundation of Apache Spark ecosystem and libraries.

Spark is composed of lower level APIs and the Structured APIs, and then series of standard libraries for additional functionality.

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/SparkToolKit.PNG?raw=true "Spark Basic Architecture")

Spark's libraries support a variety of different tasks from graph analysis and machine learning to streaming and integrations with different computing and storage systems.

## Running Production Applications  
We can run the Spark applications using `spark-submit` command line interface. In this, we can package the application using the Spark supported language such as Scala, Java, Python, and R and submit it. Spark application run in local mode or can be submitted to Spark cluster manager like Standalone, Mesos, and YARN.

`spark-submit` provides various options with which we can specify the resources required by our application.

We will run below example that is packaged with Spark which calculates the digits of `pi` to certain level of estimation. Here we are running the application in local mode with main class to run from the jar file.

// Scala Example
```
spark-submit --class org.apache.spark.examples.SparkPi --master local C:\\spark\\examples\\jars\\spark-examples_2.11-2.2.1.jar 10

Pi is roughly 3.1437311437311437
```

// Python example
```
spark-submit --master local C:\\spark\\examples\\src\\main\\python\\pi.py 10

Pi is roughly 3.1437311437311437
```

By changing the `master` argument of spark-submit, we can submit the same application to cluster running Sparks's standalone cluster manager, Mesos or YARN.

## Datasets: Type-Safe Structured APIs
Datasets is a type-safe version of Spark's structured API for writing statically typed code in Java and Scala. The Dataset API is not available in Python and R, because those languages are dynamically typed.

As we know, DataFrames are a distributed collection of objects of type `Row` that can hold various types of tabular data. The Dataset API gives users ability to assign a Java/Scala class to records within a DataFrame and manipulate it as a collection of typed objects, similar to Java `ArrayList` or Scala `Seq`.

Type-safe means, it gives our data a well defined interface. We can not view objects in Dataset as being of another class than the class you put in initially.

`Dataset` class is parameterized with the type of object contained inside: `Dataset<T>` in Java and `Dataset[T]` in Scala.

These types are restricted because Spark needs to be able to automatically analyze the type T and create an appropriate schema for tabular data within the Dataset.

One great thing about the Datasets is that we can use them only when we need them. For example- We can define our own datatype and manipulate it via arbitary map and filter functions. After we have performed our manipulations, Spark can automatically turn it back into a DataFrame, and we can manipulate it further by using the hundreads of functions that Spark includes. This makes it easy to drop down to lower level, perform type-safe coding when necessary, and move higher up to SQL for more rapid analysis.

Here is a example showing how we can use both type-safe functions and DataFrame like SQL expression to write business logic -

```
import spark.implicits._

case class Flight(DEST_COUNTRY_NAME: String, ORIGIN_COUNTRY_NAME: String, count: BigInt)
val flightsDF = spark.read.parquet("D:\\Github\\ApacheSpark\\Certification-Notes\\data\\flight-data\\parquet\\2010-summary.parquet\\")
val flights = flightsDF.as[Flight]


import spark.implicits._
defined class Flight
flightsDF: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]
flights: org.apache.spark.sql.Dataset[Flight] = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]
```

When we call `collect` or `take` on a Dataset, it will collect objects of the proper type in Dataset, not DataFrame `Rows`. This makes it easy to get type safety and securely perform manipulation in distributed and local manner without code changes

// in Scala
```
flights
  .filter(flight_row => flight_row.ORIGIN_COUNTRY_NAME != "Canada")
  .map(flight_row => flight_row)
  .take(5)

res23: Array[Flight] = Array(Flight(United States,Romania,1), Flight(United States,Ireland,264), Flight(United States,India,69), Flight(Egypt,United States,24), Flight(Equatorial Guinea,United States,1))
```

## Structured Streaming
Structure Streaming is a high-level API for stream processing that became production-ready in Spark 2.2. With Structured Streaming, we can take the same operations that we perform in batch mode using Spark's structured API and run them in streaming fashion. This can reduce the latency and allow for the incremental processing.

Structured streaming allows you to rapidly and quickly extract value out of streaming systems with virtually no code changes.

For example- Lets take example of time-series retail dataset, one that has specific dates and times for us to be able to use. One file represents one day of data. It is in this format to simulate data being produced in a consistent and regular manner by a different process.

We can imagine this data being produced by retail stores and sent to a location where they will be read by our Structured Streaming Job.

We will first analyze data as a static dataset and create a DataFrame. Then, we will convert the same program to streaming.

In below example, we will take a look at the sale hours during which a given customer makes a large purchase. We will add a total_cost column and see on what days a customer spent the most.

The `window` function will include all data from each day in the aggregation. It's a simply a window over the time-series column(InvoiceDate) in our data. This is helpful tool for manipulating date and timestamps because we can specify our requirenments in a more human form, and Spark will group all them together for us:

We will change the default Shuffle partition from 200 to 5. This configuration specifies the number of partitions that should be created after a shuffle.

```
package org.apache.spark.chapter3

import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.spark.sql._
import org.apache.spark.sql.functions.{window, column, col, desc}
import org.apache.log4j._

object StaticDataFrame {
  
  def main(args: Array[String]) {
   
    // Create Spark Session
    val spark = SparkSession
      .builder()
      .appName("StaticDataFrame")
      .master("local[*]")
      .getOrCreate()
      
    // Change the default shuffle partition from 200 to 5
    spark.conf.set("spark.sql.shuffle.partition", "5")
    
    // Read the data from CSV file
    val staticDataFrame = spark.read.format("csv")
      .option("header", "true")
      .option("inferSchema", "true")
      .load("D:\\Github\\ApacheSpark\\Certification-Notes\\data\\retail-data\\by-day\\*.csv")
      
    // Create a temporary table
    staticDataFrame.createOrReplaceTempView("retail_data")
    val staticSchema = staticDataFrame.schema
    
    // Read the time series data 1 day at a time and show first 5 rows
    staticDataFrame
      .selectExpr(
        "CustomerID",
        "(UnitPrice * Quantity) as total_cost",
        "InvoiceDate")
      .groupBy(col("CustomerID"), window(col("InvoiceDate"),"1 day"))
      .sum("total_cost")
      .show(5)
    
  }
}
```

We will convert the above static data frame based job to streaming code. The biggest change is that we used `readStream` instead of `read`. We have also added `maxFilesPerTrigger` option which simply specified the number of files we should read in at once.

Streaming actions are a bit different from our conventional static action because we are going to be populating data somewhere instead of just calling something like `count`. The action we will use will output to an in-memory table that will update after each `trigger`. In this case, each trigger is based on the individual file.

Spark will mutate the data in the in-memory table such data we will always have the highest value as specified in our previous aggregation.

```
package org.apache.spark.chapter3

import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.spark.sql._
import org.apache.spark.sql.functions.{window, column, col, desc}
import org.apache.log4j._

object StructuredStreaming {

  def main(args: Array[String]) {
   
    // Create Spark Session
    val spark = SparkSession
      .builder()
      .appName("StaticDataFrame")
      .master("local[*]")
      .getOrCreate()
      
    // Change the default shuffle partition from 200 to 5
    spark.conf.set("spark.sql.shuffle.partition", "5")
    
    // Read the data from CSV file
    val staticDataFrame = spark.read.format("csv")
      .option("header", "true")
      .option("inferSchema", "true")
      .load("D:\\Github\\ApacheSpark\\Certification-Notes\\data\\retail-data\\by-day\\*.csv")
      
    // Create a schema
    val staticSchema = staticDataFrame.schema
    
    // Create Streaming DataFrame and Read the data from CSV file
    val streamingDataFrame = spark.readStream.format("csv")
      .schema(staticSchema)
      .option("maxFilesPerTrigger", 1)
      .option("header", "true")
      .load("D:\\Github\\ApacheSpark\\Certification-Notes\\data\\retail-data\\by-day\\*.csv") 
      
    // Check if DataFrame is Streaming
    println("Is DataFrame Streaming", streamingDataFrame.isStreaming)
    
    // Read the time series data 1 day at a time and calculate purchase by hour
    val purchaseByCustomerPerHour = staticDataFrame
      .selectExpr(
        "CustomerID",
        "(UnitPrice * Quantity) as total_cost",
        "InvoiceDate")
      .groupBy(col("CustomerID"), window(col("InvoiceDate"),"1 day"))
      .sum("total_cost")
      
    // Call Streaming Action
    purchaseByCustomerPerHour.writeStream
      .format("memory")  // memory = store in-memory table
      .queryName("customer_purchases")  // the name of the in-memory table
      .outputMode("complete")  // complete - All the counts should be in table
      .start()
    
  }
}
```

When we start the strem, we can run queries against output table to debug what our result will look lile if we were to write this out to a production sink.

```
// in Scala
spark.sql("""
  SELECT *
  FROM customer_purchases
  ORDER BY `sum(total_cost)` DESC
  """)
  .show(5)
```

Another, option we can use this to write the results out to the console:

```
purchaseByCustomerPerHour.writeStream
    .format("console")  // console = write to console
    .queryName("customer_purchases")  // the name of the in-memory table
    .outputMode("complete")  // complete - All the counts should be in table
    .start()
```

We should not use either of the streaming methods in production. Notice that, `window` is built on the event time, and not the time at which Spark processes the data. This was one of the shortcommings of Spark Streaming that Structured Streaming has resolved.

## Machine Learning and Advanced Analytics
Apache Spark has ability to perform large-scale machine learning with built in library of machine learning algorithms called MLlib. MLlib allows for preprocessing, munging, training of models, amd making predictions at scale on data. We can even use models trained in MLLib to make predictions in Structured Streaming.

Spark provides sophisticated machine learning API for performing a variety of machine learning tasks, from classification to regression, and clustering to deep learning.

In below example, we will perform some basic clustering on our data usings standard algorithms called `k-means`.

**k-means**  
k-means is a clustering algorithm in which "k" centers are randomly assigned within the data. The points closest to that point are them "assigned" to a class and the center of the assigned points is computed. This center point is called the `centroid`. We then label the points closest to that centroid, to the centroid's class, and shift the centroid to new center of that cluster of points. We repeat this process for a finite set of iterations or untill convergenace(our center points stop changing).

Spark includes a number of preprocessing methods out of the box. To demonstrate these methods, we will begin with some raw data, build up transformations before getting the data into the right format, at which point we can actually train our models and then serve predictions:

Machine learning algorithms in MLlib requires that data is represented as numerical values. Out current data is represented by a variety of different types, including time stamps, integer, and strings. Thereforre, we need to transform this data into some numerical representations.

```
package org.apache.spark.chapter3

import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.spark.sql._
import org.apache.spark.sql.functions.{window, column, col, desc}
import org.apache.log4j._

object MachineLearning {

  def main(args: Array[String]) {
   
    // Create Spark Session
    val spark = SparkSession
      .builder()
      .appName("StaticDataFrame")
      .master("local[*]")
      .getOrCreate()
      
    // Change the default shuffle partition from 200 to 5
    spark.conf.set("spark.sql.shuffle.partition", "5")
    
    // Read the data from CSV file
    val staticDataFrame = spark.read.format("csv")
      .option("header", "true")
      .option("inferSchema", "true")
      .load("D:\\Github\\ApacheSpark\\Certification-Notes\\data\\retail-data\\by-day\\*.csv")
      
      
    // Convert the data into numerical format which will understood by Machine Learning Models
    import org.apache.spark.sql.functions.date_format
    val preppedDataFrame = staticDataFrame
      .na.fill(0)
      .withColumn("day_of_week", date_format(col("InvoiceDate"), "EEEE"))
      .coalesce(5)
      
    // Divide the data to create training data and test data
    val trainDataFrame = preppedDataFrame.where("InvoiceDate < '2011-07-01'")
    val testDataFrame = preppedDataFrame.where("InvoiceDate >= '2011-07-01'")
    
    // Check the count
    trainDataFrame.count()
    testDataFrame.count()

    // ML transformation which converts the string to index. It can assign the index to days of week like Monday-1, Friday-5
    import org.apache.spark.ml.feature.StringIndexer
    val indexer = new StringIndexer()
      .setInputCol("day_of_week")
      .setOutputCol("day_of_week_index")
  
    // Encode the values to column
    import org.apache.spark.ml.feature.OneHotEncoder
    val encoder = new OneHotEncoder()
      .setInputCol("day_of_week_index")
      .setOutputCol("day_of_week_encoded")

    // All of these will result in set of columns, that we will assemble into vector. All MLLib algorithms in Spark takes Vector as input
    import org.apache.spark.ml.feature.VectorAssembler
    val vectorAssembler = new VectorAssembler()
      .setInputCols(Array("UnitPrice", "Quantity", "day_of_week_encoded"))
      .setOutputCol("features")
  
    // Ste up pipeline, so if any future data we need to transform will go through this pipeline
    import org.apache.spark.ml.Pipeline
    val transformationPipeline = new Pipeline()
      .setStages(Array(indexer, encoder, vectorAssembler))
  
    val fittedPipeline = transformationPipeline.fit(trainDataFrame)
    val transformedTraining = fittedPipeline.transform(trainDataFrame)
  
    transformedTraining.cache()

    import org.apache.spark.ml.clustering.KMeans
    val kmeans = new KMeans()
      .setK(20)
      .setSeed(1L)

    val kmModel = kmeans.fit(transformedTraining)
    kmModel.computeCost(transformedTraining)
    
    val transformedTest = fittedPipeline.transform(testDataFrame)
    kmModel.computeCost(transformedTest)  
  }
}
```

## Lower-Level APIs
Spark includes number of lower-level primitives to allow for arbitary Java and Python object manipulation via Resilient Distributed DataSets. Virtually everything in Spark is built on the top of RDDs.

DataFrames operations are built on top of RDDs and compile down to these lower-level tools for convinient and extremely efficient distributed execution.

We use RDD, when we want to manipulate raw data. One thing, that we might use RDD for is to paralelize raw data that we have stored in memory on the driver machine.