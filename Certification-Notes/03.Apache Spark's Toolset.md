## Introduction
Spark's core concepts like transformations and actions in the context of Spark's structured APIs are the conceptual building block and foundation of Apache Spark ecosystem and libraries.

Spark is composed of lower level APIs and the Structured APIs, and then series of standard libraries for additional functionality.

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/SparkToolKit.PNG?raw=true "Spark Basic Architecture")

Spark's libraries support a variety of different tasks from graph analysis and machine learning to streaming and integrations with different computing and storage systems.

## Running Production Applications  
We can run the Spark applications using `spark-submit` command line interface. In this, we can package the application using the Spark supported language such as Scala, Java, Python, and R and submit it. Spark application run in local mode or can be submitted to Spark cluster manager like Standalone, Mesos, and YARN.

`spark-submit` provides various options with which we can specify the resources required by our application.

We will run below example that is packaged with Spark which calculates the digits of `pi` to certain level of estimation. Here we are running the application in local mode with main class to run from the jar file.

// Scala Example
```
spark-submit --class org.apache.spark.examples.SparkPi --master local C:\\spark\\examples\\jars\\spark-examples_2.11-2.2.1.jar 10

Pi is roughly 3.1437311437311437
```

// Python example
```
spark-submit --master local C:\\spark\\examples\\src\\main\\python\\pi.py 10

Pi is roughly 3.1437311437311437
```

By changing the `master` argument of spark-submit, we can submit the same application to cluster running Sparks's standalone cluster manager, Mesos or YARN.

## Datasets: Type-Safe Structured APIs
Datasets is a type-safe version of Spark's structured API for writing statically typed code in Java and Scala. The Dataset API is not available in Python and R, because those languages are dynamically typed.

As we know, DataFrames are a distributed collection of objects of type `Row` that can hold various types of tabular data. The Dataset API gives users ability to assign a Java/Scala class to records within a DataFrame and manipulate it as a collection of typed objects, similar to Java `ArrayList` or Scala `Seq`.

Type-safe means, it gives our data a well defined interface. We can not view objects in Dataset as being of another class than the class you put in initially.

`Dataset` class is parameterized with the type of object contained inside: `Dataset<T>` in Java and `Dataset[T]` in Scala.

These types are restricted because Spark needs to be able to automatically analyze the type T and create an appropriate schema for tabular data within the Dataset.

One great thing about the Datasets is that we can use them only when we need them. For example- We can define our own datatype and manipulate it via arbitary map and filter functions. After we have performed our manipulations, Spark can automatically turn it back into a DataFrame, and we can manipulate it further by using the hundreads of functions that Spark includes. This makes it easy to drop down to lower level, perform type-safe coding when necessary, and move higher up to SQL for more rapid analysis.

Here is a example showing how we can use both type-safe functions and DataFrame like SQL expression to write business logic -

```
import spark.implicits._

case class Flight(DEST_COUNTRY_NAME: String, ORIGIN_COUNTRY_NAME: String, count: BigInt)
val flightsDF = spark.read.parquet("D:\\Github\\ApacheSpark\\Certification-Notes\\data\\flight-data\\parquet\\2010-summary.parquet\\")
val flights = flightsDF.as[Flight]


import spark.implicits._
defined class Flight
flightsDF: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]
flights: org.apache.spark.sql.Dataset[Flight] = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]
```

When we call `collect` or `take` on a Dataset, it will collect objects of the proper type in Dataset, not DataFrame `Rows`. This makes it easy to get type safety and securely perform manipulation in distributed and local manner without code changes

// in Scala
```
flights
  .filter(flight_row => flight_row.ORIGIN_COUNTRY_NAME != "Canada")
  .map(flight_row => flight_row)
  .take(5)

res23: Array[Flight] = Array(Flight(United States,Romania,1), Flight(United States,Ireland,264), Flight(United States,India,69), Flight(Egypt,United States,24), Flight(Equatorial Guinea,United States,1))
```

## Structured Streaming
Structure Streaming is a high-level API for stream processing that became production-ready in Spark 2.2. With Structured Streaming, we can take the same operations that we perform in batch mode using Spark's structured API and run them in streaming fashion. This can reduce the latency and allow for the incremental processing.

Structured streaming allows you to rapidly and quickly extract value out of streaming systems with virtually no code changes.

For example- Lets take example of time-series retail dataset, one that has specific dates and times for us to be able to use. One file represents one day of data. It is in this format to simulate data being produced in a consistent and regular manner by a different process.

We can imagine this data being produced by retail stores and sent to a location where they will be read by our Structured Streaming Job.

We will first analyze data as a static dataset and create a DataFrame. Then, we will convert the same program to streaming.

In below example, we will take a look at the sale hours during which a given customer makes a large purchase. We will add a total_cost column and see on what days a customer spent the most.

The `window` function will include all data from each day in the aggregation. It's a simply a window over the time-series coumn(InvoiceDate) in our data. This is helpful tool for manipulating date and timestamps because we can specify our requirenments in a more human form, and Spark will group all them together for us:

We will change the default Shuffle partition from 200 to 5. This configuration specifies the number of partitions that should be created after a shuffle.

```
package org.apache.spark.chapter3

import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.spark.sql._
import org.apache.spark.sql.functions.{window, column, col, desc}
import org.apache.log4j._

object StaticDataFrame {
  
  def main(args: Array[String]) {
   
    // Create Spark Session
    val spark = SparkSession
      .builder()
      .appName("StaticDataFrame")
      .master("local[*]")
      .getOrCreate()
      
    // Change the default shuffle partition from 200 to 5
    spark.conf.set("spark.sql.shuffle.partition", "5")
    
    // Read the data from CSV file
    val staticDataFrame = spark.read.format("csv")
      .option("header", "true")
      .option("inferSchema", "true")
      .load("D:\\Github\\ApacheSpark\\Certification-Notes\\data\\retail-data\\by-day\\*.csv")
      
    // Create a temporary table
    staticDataFrame.createOrReplaceTempView("retail_data")
    val staticSchema = staticDataFrame.schema
    
    // Read the time series data 1 day at a time and show first 5 rows
    staticDataFrame
      .selectExpr(
        "CustomerID",
        "(UnitPrice * Quantity) as total_cost",
        "InvoiceDate")
      .groupBy(col("CustomerID"), window(col("InvoiceDate"),"1 day"))
      .sum("total_cost")
      .show(5)
    
  }
}
```

We will convert the above static data frame based job to streaming code. The biggest change is that we used `readStream` instead of `read`. We have also added `maxFilesPerTrigger` option which simply specified the number of files we should read in at once.

Streaming actions are a bit different from our conventional static action because we are going to be populating data somewhere instead of just calling something like `count`. The action we will use will output to an in-memory table that will update after each `trigger`. In this case, each trigger is based on the individual file.

Spark will mutate the data in the in-memory table such data we will always have the highest value as specified in our previous aggregation.

```
package org.apache.spark.chapter3

import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.spark.sql._
import org.apache.spark.sql.functions.{window, column, col, desc}
import org.apache.log4j._

object StructuredStreaming {

  def main(args: Array[String]) {
   
    // Create Spark Session
    val spark = SparkSession
      .builder()
      .appName("StaticDataFrame")
      .master("local[*]")
      .getOrCreate()
      
    // Change the default shuffle partition from 200 to 5
    spark.conf.set("spark.sql.shuffle.partition", "5")
    
    // Read the data from CSV file
    val staticDataFrame = spark.read.format("csv")
      .option("header", "true")
      .option("inferSchema", "true")
      .load("D:\\Github\\ApacheSpark\\Certification-Notes\\data\\retail-data\\by-day\\*.csv")
      
    // Create a schema
    val staticSchema = staticDataFrame.schema
    
    // Create Streaming DataFrame and Read the data from CSV file
    val streamingDataFrame = spark.readStream.format("csv")
      .schema(staticSchema)
      .option("maxFilesPerTrigger", 1)
      .option("header", "true")
      .load("D:\\Github\\ApacheSpark\\Certification-Notes\\data\\retail-data\\by-day\\*.csv") 
      
    // Check if DataFrame is Streaming
    println("Is DataFrame Streaming", streamingDataFrame.isStreaming)
    
    // Read the time series data 1 day at a time and calculate purchase by hour
    val purchaseByCustomerPerHour = staticDataFrame
      .selectExpr(
        "CustomerID",
        "(UnitPrice * Quantity) as total_cost",
        "InvoiceDate")
      .groupBy(col("CustomerID"), window(col("InvoiceDate"),"1 day"))
      .sum("total_cost")
      
    // Call Streaming Action
    purchaseByCustomerPerHour.writeStream
      .format("memory")  // memory = store in-memory table
      .queryName("customer_purchases")  // the name of the in-memory table
      .outputMode("complete")  // complete - All the counts should be in table
      .start()
    
  }
}
```

When we start the strem, we can run queries against it to debug what our result will look lile if we were to write this out to a production sink.

```
// in Scala
spark.sql("""
  SELECT *
  FROM customer_purchases
  ORDER BY `sum(total_cost)` DESC
  """)
  .show(5)
```

Another, option we can use this to write the results out to the console:

```
purchaseByCustomerPerHour.writeStream
    .format("console")  // console = write to console
    .queryName("customer_purchases")  // the name of the in-memory table
    .outputMode("complete")  // complete - All the counts should be in table
    .start()
```

We should not use either of the streaming methods in production. Notice that, `window` is built on the event time, and not the time at which Spark processes the data. This was one of the shortcommings of Spark Streaming that Structured Streaming has resolved.

## Machine Learning and Advanced Analytics