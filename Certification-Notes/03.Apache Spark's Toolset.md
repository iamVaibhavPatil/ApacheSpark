## Introduction
Spark's core concepts like transformations and actions in the context of Spark's structured APIs are the conceptual building block and foundation of Apache Spark ecosystem and libraries.

Spark is composed of lower level APIs and the Structured APIs, and then series of standard libraries for additional functionality.

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/SparkToolKit.PNG?raw=true "Spark Basic Architecture")

Spark's libraries support a variety of different tasks from graph analysis and machine learning to streaming and integrations with different computing and storage systems.

## Running Production Applications  
We can run the Spark applications using `spark-submit` command line interface. In this, we can package the application using the Spark supported language such as Scala, Java, Python, and R and submit it. Spark application run in local mode or can be submitted to Spark cluster manager like Standalone, Mesos, and YARN.

`spark-submit` provides various options with which we can specify the resources required by our application.

We will run below example that is packaged with Spark which calculates the digits of `pi` to certain level of estimation. Here we are running the application in local mode with main class to run from the jar file.

// Scala Example
```
spark-submit --class org.apache.spark.examples.SparkPi --master local C:\\spark\\examples\\jars\\spark-examples_2.11-2.2.1.jar 10

Pi is roughly 3.1437311437311437
```

// Python example
```
spark-submit --master local C:\\spark\\examples\\src\\main\\python\\pi.py 10

Pi is roughly 3.1437311437311437
```

By changing the `master` argument of spark-submit, we can submit the same application to cluster running Sparks's standalone cluster manager, Mesos or YARN.

## Datasets: Type-Safe Structured APIs
Datasets is a type-safe version of Spark's structured API for writing statically typed code in Java and Scala. The Dataset API is not available in Python and R, because those languages are dynamically typed.

As we know, DataFrames are a distributed collection of objects of type `Row` that can hold various types of tabular data. The Dataset API gives users ability to assign a Java/Scala class to records within a DataFrame and manipulate it as a collection of typed objects, similar to Java `ArrayList` or Scala `Seq`.

Type-safe means, it gives our data a well defined interface. We can not view objects in Dataset as being of another class than the class you put in initially.

`Dataset` class is parameterized with the type of object contained inside: `Dataset<T>` in Java and `Dataset[T]` in Scala.

These types are restricted because Spark needs to be able to automatically analyze the type T and create an appropriate schema for tabular data within the Dataset.

One great thing about the Datasets is that we can use them only when we need them. For example- We can define our own datatype and manipulate it via arbitary map and filter functions. After we have performed our manipulations, Spark can automatically turn it back into a DataFrame, and we can manipulate it further by using the hundreads of functions that Spark includes. This makes it easy to drop down to lower level, perform type-safe coding when necessary, and move higher up to SQL for more rapid analysis.

Here is a example showing how we can use both type-safe functions and DataFrame like SQL expression to write business logic -

```
import spark.implicits._

case class Flight(DEST_COUNTRY_NAME: String, ORIGIN_COUNTRY_NAME: String, count: BigInt)
val flightsDF = spark.read.parquet("D:\\Github\\ApacheSpark\\Certification-Notes\\data\\flight-data\\parquet\\2010-summary.parquet\\")
val flights = flightsDF.as[Flight]


import spark.implicits._
defined class Flight
flightsDF: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]
flights: org.apache.spark.sql.Dataset[Flight] = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]
```

When we call `collect` or `take` on a Dataset, it will collect objects of the proper type in Dataset, not DataFrame `Rows`. This makes it easy to get type safety and securely perform manipulation in distributed and local manner without code changes

// in Scala
```
flights
  .filter(flight_row => flight_row.ORIGIN_COUNTRY_NAME != "Canada")
  .map(flight_row => flight_row)
  .take(5)

res23: Array[Flight] = Array(Flight(United States,Romania,1), Flight(United States,Ireland,264), Flight(United States,India,69), Flight(Egypt,United States,24), Flight(Equatorial Guinea,United States,1))
```

## Structured Streaming
