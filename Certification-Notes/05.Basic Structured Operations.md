## Introduction
DataFrame consists of a series of records that are of type `Row`, and a number of `columns` that represent a compuations expression that can be performed on each individual records in the Dataset.

`Schemas` define the name as well as the type of data in each column.

`Partitioning` of the DataFrame defines the layout of the DataFrame or DataSet's physical distribution across the cluster. The `partitioning scheme` defines how that is allocated. We can set this to be based on the values in a certain column or nondeterministically.

For example- Lets create a DataFrame and print schema

// in Scala
```
val df = spark.read.format("json")
  .load("D:\\Github\\ApacheSpark\\Certification-Notes\\data\\flight-data\\json\\2015-summary.json")


scala> df.printSchema()

root
 |-- DEST_COUNTRY_NAME: string (nullable = true)
 |-- ORIGIN_COUNTRY_NAME: string (nullable = true)
 |-- count: long (nullable = true)
```

// in Python
```
df = spark.read.format("json").load("D:\\Github\\ApacheSpark\\Certification-Notes\\data\\flight-data\\json\\2015-summary.json")
```

## Schemas  
A schema defines the column names and type of a DataFrame. We can either let a data source define the schema called `schema-on-read` or we can define it explicitly ourselves.








![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/scala_types.PNG?raw=true "Scala Language Types")

