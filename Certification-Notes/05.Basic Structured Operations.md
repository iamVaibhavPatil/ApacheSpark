## Introduction
DataFrame consists of a series of records that are of type `Row`, and a number of `columns` that represent a compuations expression that can be performed on each individual records in the Dataset.

`Schemas` define the name as well as the type of data in each column.

`Partitioning` of the DataFrame defines the layout of the DataFrame or DataSet's physical distribution across the cluster. The `partitioning scheme` defines how that is allocated. We can set this to be based on the values in a certain column or nondeterministically.

For example- Lets create a DataFrame and print schema

// in Scala
```
val df = spark.read.format("json")
  .load("D:\\Github\\ApacheSpark\\Certification-Notes\\data\\flight-data\\json\\2015-summary.json")


scala> df.printSchema()

root
 |-- DEST_COUNTRY_NAME: string (nullable = true)
 |-- ORIGIN_COUNTRY_NAME: string (nullable = true)
 |-- count: long (nullable = true)
```

// in Python
```
df = spark.read.format("json").load("D:\\Github\\ApacheSpark\\Certification-Notes\\data\\flight-data\\json\\2015-summary.json")
```

## Schemas  
A schema defines the column names and type of a DataFrame. We can either let a data source define the schema called `schema-on-read` or we can define it explicitly ourselves.

Deciding whether we need to define a schema prior to reading in the data depends on use case. For ad hoc analysis, schema-on-read usually works just fine. However, this can lead to precision issues like a long type incorrectly set as a integer when reading in a file.

While using Spark production Extract Transform and Load(ETL), it is often good idea to define schemas manually, especially when working with untyped data sources like CSV and JSON because schema inference can vary depending on the type data that we read in.

// in Scala
```
scala> spark.read.format("json").load("D:\\Github\\ApacheSpark\\Certification-Notes\\data\\flight-data\\json\\2015-summary.json").schema

res4: org.apache.spark.sql.types.StructType = StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,true))

```

// in Python
```
scala> spark.read.format("json").load("D:\\Github\\ApacheSpark\\Certification-Notes\\data\\flight-data\\json\\2015-summary.json").schema

res4: org.apache.spark.sql.types.StructType = StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,true)))
```

A schema is a `StructType` made up of a number of fields, `StructFields`, that have a name, type, a Boolean flag which specifies whether that column can contain missing or `null` values, and, finally, users can optionally specify associated metadata with that column. The metadata is a way of storing information about this column. Spark uses this in it's machine learning library.

Schemas can contain other StructType(Spark's complex types). If the types in the data does not match the schema at runtime, Spark will throw an error.

Below example shows how to create and enforce a specific schema on a DataFrame.

// in Scala
```
package org.apache.spark.chapter5

import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.spark.sql._
import org.apache.log4j._
import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}
import org.apache.spark.sql.types.Metadata

object SchemaExample {

  def main(args: Array[String]) {
   
    // Create Spark Session
    val spark = SparkSession
      .builder()
      .appName("StaticDataFrame")
      .master("local[*]")
      .getOrCreate()
      
    // Change the default shuffle partition from 200 to 5
    spark.conf.set("spark.sql.shuffle.partition", "5")
    
    val myManualSchema = StructType(Array(
        StructField("DEST_COUNTRY_NAME", StringType, true),
        StructField("ORINGIN_COUNTRY_NAME", StringType, true),
        StructField("count", LongType, false, Metadata.fromJson("{\"hello\":\"world\"}"))
        ))
    
    val df = spark.read.format("json").schema(myManualSchema).load("D:\\Github\\ApacheSpark\\Certification-Notes\\data\\flight-data\\json\\2015-summary.json") 
    
    df.printSchema()  
  }
}

>>>
root
 |-- DEST_COUNTRY_NAME: string (nullable = true)
 |-- ORINGIN_COUNTRY_NAME: string (nullable = true)
 |-- count: long (nullable = true)

```

// in Python
```
from pyspark.sql.types import StructField, StructType, StringType, LongType

myManualSchema = StructType([
  StructField("DEST_COUNTRY_NAME", StringType(), True),
  StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
  StructField("count", LongType(), False, metadata={"hello":"world"})
])
df = spark.read.format("json").schema(myManualSchema)\
  .load("/data/flight-data/json/2015-summary.json")
```

## Columns and Expressions
Columns in Spark are similar to columns in a spreadsheet, R dataframe, or pandas DataFrame. We can select, manipulate, and remove columns from DataFrames and those operations are represented as `expressions`

### Columns
We can construct and refer column by using `col` or `column`.

// in Scala
```
import org.apache.spark.sql.functions.{col, column}
col("someColumnName")
column("someColumnName")
```

//in python
```
from pyspark.sql.functions import col, column
col("someColumnName")
column("someColumnName")
```

This column might or might not exist in our DataFrames. Columns are not resolved untill we compare the column with those we are maintaining in the `Catalog`. Column and table resolution happens in the analyzer phase.

**Scala** has some unique language feature which allows us to $ symbol to designate a string as s epcial string that should refer to an expression.

// in Scala
```
$"myColumn"
'myColumn
```

**Explicit column references** - If we need to refer to specific DataFrame column, we can use `col` method on the specific DataFrame. This can be usefull when we are performing a join and need to refer to specific colum in one DataFrame and might share a name with another column in joined DataFrame.

### Expression
An expression is a set of transformations on one or more values in a record in a DataFrame. Think of it like a function that takes an input one or more column names, resolve them, and then potentially applies more expressions to create a single value for each record in the dataset.
This single value can be a complex type like `Map or Array`.

An expression created via `expr` is a DataFrame column reference. `expr("someColumn")` is equivalent to `col("someColumn")`.

**Columns as expressions**  
Columns provide a subset of expression functionality. If we use `col()` and want to perform transformations on that column, we must perform those on that column reference.

When using expression, the `expr` function can actually parse transformations and column references from a string and can subsequently be passed into further transformations.

`expr("someColumn - 5")` is same transformation as performing `col(someColumn) - 5`, or even `expr("someColumn") - 5`. That is because Spark compiles these to a logical tree specifying the order of operations.

Let's convert below expression into logical tree as-

```
(((col("someCol") + 5) * 200) - 6) < col("otherCol")
```

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/logical_tree.PNG?raw=true "Logical Tree")

This might look familiar because it's directed acyclic graph. This graph is represented equivalently by following code-

// in Scala
```
import org.apache.spark.sql.functions.expr
expr("(((someCol + 5) * 200) - 6) < otherCol")
```

// in Python
```
from pyspark.sql.functions import expr
expr("(((someCol + 5) * 200) - 6) < otherCol")
```

**Accesing a DataFrames Column**  
We can check the schema using `printSchema`, but sometimes if we want to programmatically access columns, we can use the `columns` property to see all the columns on a DataFrame:

```
spark.read.format("json").load("/data/flight-data/json/2015-summary.json")
  .columns
```

## Records and Rows
In Spark, each row in a DataFrame is a single record. Spark represents this record as an object of type `Row`. Spark manipulates `Row` objects using column expressions in order to produce usable values.

Row objects internally represent arrays of bytes. The byte array interface is never shown to users because we only use column expressions to manipulate them.

**Creating Rows**  
We can create rows by manually instantiating a `Row` object with values that belong in each column. Only DataFrames have schemas. Rows themselves do not have schemas. This means, if we create Row manually, we need to specify the values in the same order as schema of the DataFrame to which they might be appended.

// in Scala
```
import org.apache.spark.sql.Row
val myRow = Row("Hello", null, 1, false)
```

// in Python
```
from pyspark.sql import Row
myRow = Row("Hello", None, 1, False)
```

While accesing we just need to specify the position. In Scala or Java, we must use helper method or explicitly coerce the values. Python and R will automatically coerced to correct type:

// in Scala
```
myRow(0) // type Any
myRow(0).asInstanceOf[String] // String
myRow.getString(0) // String
myRow.getInt(2) // Int
```

// in Python
```
myRow[0]
myRow[2]
```

## DataFrame Transformations
When we work with DataFrames, there are some fundamental objectives. These break down into several core operations.

1. We can add rows or columns  
2. We can remove rows or columns  
3. We can transform a row into a column or vice-versa  
4. We can change the order of rows based on the values in columns  

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/different_kind_of_transformations.PNG?raw=true "Different kinds of transformations")

### Creating DataFrames
We can create DataFrames from raw data sources like CSV, JSON Parquet files. We can also create DataFrames and register them as View. So, later on we can query them-  

// in Scala
```
val df = spark.read.format("json")
  .load("/data/flight-data/json/2015-summary.json")
df.createOrReplaceTempView("dfTable")
```

//in Python
```
df = spark.read.format("json").load("/data/flight-data/json/2015-summary.json")
df.createOrReplaceTempView("dfTable")
```

We can also create DataFrames on fly by taking a set of rows and converting them to a DataFrame.

```
package org.apache.spark.chapter5

import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.spark.sql._
import org.apache.log4j._
import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}
import org.apache.spark.sql.types.Metadata

object CustomDataFrame {

  def main(args: Array[String]) {
   
    // Create Spark Session
    val spark = SparkSession
      .builder()
      .appName("SchemaExample")
      .master("local[*]")
      .getOrCreate()
      
    // Change the default shuffle partition from 200 to 5
    spark.conf.set("spark.sql.shuffle.partition", "5")
    
    val myManualSchema = StructType(Array(
        StructField("FIRST_NAME", StringType, true),
        StructField("LAST_NAME", StringType, true),
        StructField("EMP_ID", LongType, false, Metadata.fromJson("{\"hello\":\"world\"}"))
        ))
        
    val myRows = Seq(Row("Vaibhav",null,1571L))
    val myRDD = spark.sparkContext.parallelize(myRows)
    val myDF = spark.createDataFrame(myRDD, myManualSchema)
    myDF.show()
  }  
}

>>>
+----------+---------+------+
|FIRST_NAME|LAST_NAME|EMP_ID|
+----------+---------+------+
|   Vaibhav|     null|  1571|
+----------+---------+------+
```

// in Python
```
from pyspark.sql import Row
from pyspark.sql.types import StructField, StructType, StringType, LongType
myManualSchema = StructType([
  StructField("some", StringType(), True),
  StructField("col", StringType(), True),
  StructField("names", LongType(), False)
])
myRow = Row("Hello", None, 1)
myDf = spark.createDataFrame([myRow], myManualSchema)
myDf.show()
```

### select and selectExpr
Once we create DataFrames `select` is the most usefull method we will usng to transform these DataFrames. 

We use `select` when working with columns or expressions. We use `selectExpr` when working with expressions in strings. We can also use various functions from `org.apache.spark.sql.functions` package.

So, select and selectExpr allow us to do the DataFrame equivalent of SQL queries on the table of data:

```
SELECT * FROM dataFrameTable
SELECT columnName FROM dataFrameTable
SELECT columnName * 10, otherColumn, someOtherCol as c FROM dataFrameTable
```

We can use select and selectExpr to manipulate column in DataFrames. The easiest way is to use `select` method and pass the column names as string with which we want to work-

// in Scala
```
df.select("DEST_COUNTRY_NAME").show(2)
```

// in Python
```
df.select("DEST_COUNTRY_NAME").show(2)
```

// in SQL
```
SELECT DEST_COUNTRY_NAME FROM dfTable LIMIT 2
```

Output will be-
```
+-----------------+
|DEST_COUNTRY_NAME|
+-----------------+
|    United States|
|    United States|
+-----------------+
only showing top 2 rows
```

We can select multiple column by using the same style query-

// in Scala
```
df.select("DEST_COUNTRY_NAME","ORINGIN_COUNTRY_NAME").show(2)
```

// in Python
```
df.select("DEST_COUNTRY_NAME", "ORIGIN_COUNTRY_NAME").show(2)
```

// in SQL
```
SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME FROM dfTable LIMIT 2
```

Giving output-
```
+-----------------+--------------------+
|DEST_COUNTRY_NAME|ORINGIN_COUNTRY_NAME|
+-----------------+--------------------+
|    United States|             Romania|
|    United States|             Croatia|
+-----------------+--------------------+
only showing top 2 rows
```

We can refer to column using number of different ways-

// in Scala
```
import org.apache.spark.sql.functions.{expr, col, column}
df.select(
    df.col("DEST_COUNTRY_NAME"),
    col("DEST_COUNTRY_NAME"),
    column("DEST_COUNTRY_NAME"),
    'DEST_COUNTRY_NAME,
    $"DEST_COUNTRY_NAME",
    expr("DEST_COUNTRY_NAME"))
  .show(2)
```

// in Python
```
from pyspark.sql.functions import expr, col, column
df.select(
    expr("DEST_COUNTRY_NAME"),
    col("DEST_COUNTRY_NAME"),
    column("DEST_COUNTRY_NAME"))\
  .show(2)
```

But, we should not mix the `Column` object and strings. It will result in compile error-

```
df.select(col("DEST_COUNTRY_NAME"),"DEST_COUNTRY_NAME")
```

`expr` is the most flexible reference. It can refer to a plain column or a string manipulation of column. Let's change the column and then change is back by using the `As` keyword and then `alias` method on the column:

// in Scala
```
df.select(expr("DEST_COUNTRY_NAME AS destination")).show(2)
```

// in Python
```
df.select(expr("DEST_COUNTRY_NAME AS destination")).show(2)
```

// in SQL
```
SELECT DEST_COUNTRY_NAME as destination FROM dfTable LIMIT 2
```

This changes the column name to "destination". We can further manipulate the result of expression as another expression-

// in Scala
```
df.select(expr("DEST_COUNTRY_NAME as destination").alias("DEST_COUNTRY_NAME"))
  .show(2)
```

// in Python
```
df.select(expr("DEST_COUNTRY_NAME as destination").alias("DEST_COUNTRY_NAME"))\
  .show(2)
```

Because `select` followed by a series of expr is such as common pattern, Spark has a shorthand for doing this efficiently: `selectExpr`. This probably the most convenient interface for everyday use:

// in Scala
```
df.selectExpr("DEST_COUNTRY_NAME as newColumnName", "DEST_COUNTRY_NAME").show(2)
```

// in Python
```
df.selectExpr("DEST_COUNTRY_NAME as newColumnName", "DEST_COUNTRY_NAME").show(2)
```

This opens up the true power of Spark. We can treat `selectExpr` as a simple way to build up complex expressions that create new DataFrames.