## Introduction
DataFrame consists of a series of records that are of type `Row`, and a number of `columns` that represent a compuations expression that can be performed on each individual records in the Dataset.

`Schemas` define the name as well as the type of data in each column.

`Partitioning` of the DataFrame defines the layout of the DataFrame or DataSet's physical distribution across the cluster. The `partitioning scheme` defines how that is allocated. We can set this to be based on the values in a certain column or nondeterministically.

For example- Lets create a DataFrame and print schema

// in Scala
```
val df = spark.read.format("json")
  .load("D:\\Github\\ApacheSpark\\Certification-Notes\\data\\flight-data\\json\\2015-summary.json")


scala> df.printSchema()

root
 |-- DEST_COUNTRY_NAME: string (nullable = true)
 |-- ORIGIN_COUNTRY_NAME: string (nullable = true)
 |-- count: long (nullable = true)
```

// in Python
```
df = spark.read.format("json").load("D:\\Github\\ApacheSpark\\Certification-Notes\\data\\flight-data\\json\\2015-summary.json")
```

## Schemas  
A schema defines the column names and type of a DataFrame. We can either let a data source define the schema called `schema-on-read` or we can define it explicitly ourselves.

Deciding whether we need to define a schema prior to reading in the data depends on use case. For ad hoc analysis, schema-on-read usually works just fine. However, this can lead to precision issues like a long type incorrectly set as a integer when reading in a file.

While using Spark production Extract Transform and Load(ETL), it is often good idea to define schemas manually, especially when working with untyped data sources like CSV and JSON because schema inference can vary depending on the type data that we read in.

// in Scala
```
scala> spark.read.format("json").load("D:\\Github\\ApacheSpark\\Certification-Notes\\data\\flight-data\\json\\2015-summary.json").schema

res4: org.apache.spark.sql.types.StructType = StructType(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,true))

```

// in Python
```
scala> spark.read.format("json").load("D:\\Github\\ApacheSpark\\Certification-Notes\\data\\flight-data\\json\\2015-summary.json").schema

res4: org.apache.spark.sql.types.StructType = StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true), StructField(ORIGIN_COUNTRY_NAME,StringType,true), StructField(count,LongType,true)))
```

A schema is a `StructType` made up of a number of fields, `StructFields`, that have a name, type, a Boolean flag which specifies whether that column can contain missing or `null` values, and, finally, users can optionally specify associated metadata with that column. The metadata is a way of storing information about this column. Spark uses this in it's machine learning library.

Schemas can contain other StructType(Spark's complex types). If the types in the data does not match the schema at runtime, Spark will throw an error.

Below example shows how to create and enforce a specific schema on a DataFrame.

// in Scala
```
package org.apache.spark.chapter5

import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.spark.sql._
import org.apache.spark.sql.functions.{window, column, col, desc}
import org.apache.log4j._
import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}
import org.apache.spark.sql.types.Metadata

object SchemaExample {

  def main(args: Array[String]) {
   
    // Create Spark Session
    val spark = SparkSession
      .builder()
      .appName("StaticDataFrame")
      .master("local[*]")
      .getOrCreate()
      
    // Change the default shuffle partition from 200 to 5
    spark.conf.set("spark.sql.shuffle.partition", "5")
    
    val myManualSchema = StructType(Array(
        StructField("DEST_COUNTRY_NAME", StringType, true),
        StructField("ORINGIN_COUNTRY_NAME", StringType, true),
        StructField("count", LongType, false, Metadata.fromJson("{\"hello\":\"world\"}"))
        ))
    
    val df = spark.read.format("json").schema(myManualSchema).load("D:\\Github\\ApacheSpark\\Certification-Notes\\data\\flight-data\\json\\2015-summary.json") 
    
    df.printSchema()  
  }
}

>>>
root
 |-- DEST_COUNTRY_NAME: string (nullable = true)
 |-- ORINGIN_COUNTRY_NAME: string (nullable = true)
 |-- count: long (nullable = true)

```

// in Python
```
from pyspark.sql.types import StructField, StructType, StringType, LongType

myManualSchema = StructType([
  StructField("DEST_COUNTRY_NAME", StringType(), True),
  StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
  StructField("count", LongType(), False, metadata={"hello":"world"})
])
df = spark.read.format("json").schema(myManualSchema)\
  .load("/data/flight-data/json/2015-summary.json")
```

## Columns and Expressions
