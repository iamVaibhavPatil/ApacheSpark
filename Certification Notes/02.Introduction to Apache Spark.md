## Spark's Basic Architecture
A single computer or laptop works perfectly well for watching movies or working with spreadsheet software or working on small datasets, but there are some things that single computer is not powerful enough to perform. 

One particularly challenging area is data processing. Single machines do not have enough power and resources to perform computation on huge amount of information. A cluster or group of computers, pools the resources of many machines together, giving us the ability to use all the cumulative resources as if they were single computer. Now, a group of machines alone is not powerful, you need a framework to coordinate work across them. Spark does just that, managing and coordinating the execution of task on data across a cluster of computers.

The cluster of machines that Spark will use to execute tasks is managed by a cluster manager. Spark support many cluster managers -

- Spark Standalone cluster manager  
- YARN  
- MESOS  

We then submit Spark applications to these cluster manager, which will grant resources to our application so that we can complete our work.

### Spark Applications  
Spark Applications consist of **a driver process** and **a set of executor processes**. 

#### Driver Process  
The driver process which sits on a node in cluster and runs the main() function is responsible for 3 things-

- Maintaining information about the Spark Applications  
- Responding to a user's program or input  
- Analyzing, distributing, and scheduling work across the executors.

The driver process is absolutely essential. It is the heart of Spark applications and maintains all the relevant information during the lifetime of the application.

#### Executor Processes
The executors are responsible for actually carrying out the work that the driver assigns them. Executors are responsible for only 2 things- 

- Executing the code assigned to them by the driver  
- Reporting the state of computation on that executor back to the driver node.  

Below diagram demonstrate how the cluster manager controls the physical machines and allocates resources to Spark Applications. There could be multiple Spark applications running on a cluster at the same time.

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/Spark_Basic_Architecture.PNG?raw=true "Spark Basic Architecture")

Spark, in addition to it's cluster mode, also has a **local mode**. The driver and executors are simply processes, which means that they can live on the same machine or different machines. In local mode, the driver and executors run(as thread) on single computer instead of a cluster.

Key points about Spark Applications-  

- Spark employs a cluster manager that keeps track of resources available  
- The driver prcess is responsible for executing the driver program's commands across the executors to complete given task.

Executors, for the most part will always be running Spark code. However, the driver can be driven from number of different language through Spark language APIs.

## Spark Language APIs
Spark supports Scala, Java, Python, SQL, R. Spark presents core concepts in every lanaguge; these concepts are them translated into Spark code that runs on the cluster of machines. If we just used Structured APIs, we can expect all languages to have similar performance characteristics.

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/SparkSessionAndSparkLanguageRel.PNG?raw=true "Relationship Between Spark Session and Spark Language")

SparkSession obejct is available to user, which is entrance point to running Spark code. When using Python or R, we dont need to write explicit JVM instructions: instead, Spark translates Python code into a code which can run on executors JVM's.

Spark have 2 fundamental set's of API -

- Low Level unstructured APIs
- Higer level structured APIs

## SparkSession
When we write the Spark Applications, we send the user commands and data to Application using SparkSession object.

### Spark using interactive console
We can start the Spark interactive console application using  `./bin/spark-shell` to acess the scala console OR `./bin/pyspark` to access Python console.

When we start the console, we get built in SparkSession object called as `spark`. You can check by typing in console

Scala Console
```
scala> spark
res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@2cea921a
```

Pyton Console
```
>>> spark
<pyspark.sql.session.SparkSession object at 0x000002009708AB38>
>>>
```

### Spark using Stand alone application
We can create standalone applications and submit them to Spark using `spark-submit` command line. When we create standalone application, we must create SparkSession object to send the commands and data to Spark.

## DataFrames
A DataFrame is the most common Structured API and simply represents a table of data with rows and columns. The list that defines the columns and the types within those columns is called the `schema`.

We can think of DataFrame as a spreadsheet with named columns. Fundamental difference is: a spreadsheet sits on one computer in one specific location, whereas a Spark DataFrame can span thousands of computers.

The reason for putting the data on more than one computer should be intuitive:either the data is too large to fit on one machine or it would simply take too long to perform that computation on one machine.

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/SparkDistributedVsSingle.PNG?raw=true "Relationship Between Spark Session and Spark Language")
