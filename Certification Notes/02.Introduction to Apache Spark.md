## Spark's Basic Architecture
A single computer or laptop works perfectly well for watching movies or working with spreadsheet software or working on small datasets, but there are some things that single computer is not powerful enough to perform. 

One particularly challenging area is data processing. Single machines do not have enough power and resources to perform computation on huge amount of information. A cluster or group of computers, pools the resources of many machines together, giving us the ability to use all the cumulative resources as if they were single computer. Now, a group of machines alone is not powerful, you need a framework to coordinate work across them. Spark does just that, managing and coordinating the execution of task on data across a cluster of computers.

The cluster of machines that Spark will use to execute tasks is managed by a cluster manager. Spark support many cluster managers -

- Spark Standalone cluster manager  
- YARN  
- MESOS  

We then submit Spark applications to these cluster manager, which will grant resources to our application so that we can complete our work.

### Spark Applications  
Spark Applications consist of **a driver process** and **a set of executor processes**. 

#### Driver Process  
The driver process which sits on a node in cluster and runs the main() function is responsible for 3 things-

- Maintaining information about the Spark Applications  
- Responding to a user's program or input  
- Analyzing, distributing, and scheduling work across the executors.

The driver process is absolutely essential. It is the heart of Spark applications and maintains all the relevant information during the lifetime of the application.

#### Executor Processes
The executors are responsible for actually carrying out the work that the driver assigns them. Executors are responsible for only 2 things- 

- Executing the code assigned to them by the driver  
- Reporting the state of computation on that executor back to the driver node.  

Below diagram demonstrate how the cluster manager controls the physical machines and allocates resources to Spark Applications. There could be multiple Spark applications running on a cluster at the same time.

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/Spark_Basic_Architecture.PNG?raw=true "Spark Basic Architecture")

Spark, in addition to it's cluster mode, also has a **local mode**. The driver and executors are simply processes, which means that they can live on the same machine or different machines. In local mode, the driver and executors run(as thread) on single computer instead of a cluster.

Key points about Spark Applications-  

- Spark employs a cluster manager that keeps track of resources available  
- The driver process is responsible for executing the driver program's commands across the executors to complete given task.

Executors, for the most part will always be running Spark code. However, the driver can be driven from number of different language through Spark language APIs.

## Spark Language APIs
Spark supports Scala, Java, Python, SQL, R. Spark presents core concepts in every lanaguge; these concepts are them translated into Spark code that runs on the cluster of machines. If we just used Structured APIs, we can expect all languages to have similar performance characteristics.

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/SparkSessionAndSparkLanguageRel.PNG?raw=true "Relationship Between Spark Session and Spark Language")

SparkSession obejct is available to user, which is entrance point to running Spark code. When using Python or R, we dont need to write explicit JVM instructions: instead, Spark translates Python code into a code which can run on executors JVM's.

Spark have 2 fundamental set's of API -

- Low Level unstructured APIs
- Higer level structured APIs

## SparkSession
When we write the Spark Applications, we send the user commands and data to Application using SparkSession object.

### Spark using interactive console
We can start the Spark interactive console application using  `./bin/spark-shell` to acess the scala console OR `./bin/pyspark` to access Python console.

When we start the console, we get built in SparkSession object called as `spark`. You can check by typing in console

Scala Console
```
scala> spark
res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@2cea921a
```

Pyton Console
```
>>> spark
<pyspark.sql.session.SparkSession object at 0x000002009708AB38>
>>>
```

### Spark using Stand alone application
We can create standalone applications and submit them to Spark using `spark-submit` command line. When we create standalone application, we must create SparkSession object to send the commands and data to Spark.

## DataFrames
A DataFrame is the most common Structured API and simply represents a table of data with rows and columns. The list that defines the columns and the types within those columns is called the `schema`.

We can think of DataFrame as a spreadsheet with named columns. Fundamental difference is: a spreadsheet sits on one computer in one specific location, whereas a Spark DataFrame can span thousands of computers.

The reason for putting the data on more than one computer should be intuitive:either the data is too large to fit on one machine or it would simply take too long to perform that computation on one machine.

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/SparkDistributedVsSingle.PNG?raw=true "Relationship Between Spark Session and Spark Language")

R and Python also have DataFrame concepts and they exists on one machine rather than multiple machines. Spark has language interfaces for both Python and R, it's quite easy to convert Pandas(Python) DataFrames to Spark DataFrames, and R DataFrames to Spark DataFrames.

Spark has several core abstractions: `DataSets, DataFrames, SQL Tables, and Resilient Distributed DataSets(RDDs)`. These different abstractions all represent distributed collections of data.

#### Partitions
To allow every executor to perform work in parallel, Spark breaks up the data into chunks called `Partitions`. A partition is a collection of rows that sit on one physical machine in cluster.

A DataFrame's partition represent how the data is physically distributed across the clusters of machines during execution. 

If we have one partition, Spark will have a parallelism of only one, even if we have thousands of executors. If we have many partitions but only one executor, Spark will still have a parallelism of only one because there is only one computation resource.

With DataFrames we don't need to manipulate partitions manually or individually. We simply need to specify high-level transformations of data in physical partitions, and Spark determines how this work will actully execute on clsuter.

Let's create a DataFrame with one column containing 1000 rows with values from 0 to 999. This range of numbers represents a `distributed collection`. When run on a cluster, each part of this range of numbers exists on a different exectuor.

// In Scala
```
val myRange = spark.range(1000).toDF("number")
```
myRange: org.apache.spark.sql.DataFrame = [number: bigint]


// In Python
```
myRange = spark.range(1000).toDF("number")
```

## Transformations
In Spark, the core data structures are immutable, meaning they cannot be changed after they are created. To change a DataFrame, we need to instruct Spark, how we like to modify it, these instructions are called as transformations.

For Example- A simple transformation to find all even numbers in our current DataFrame:

//In Scala
```
val divisBy2 = myRange.where("number % 2 = 0")
```

//In Python
```
divisBy2 = myRange.where("number % 2 = 0")
```

Spark will not act on transformations untill we call an action. Transformations are the core of how we express business logic using Spark. There are 2 types of transformations - 

### Narrow Tranformations  
Transformations consisting of narrow dependencies are those for which each input partition will contribute to only one output partition.

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/Narrow_Transformations.PNG?raw=true "Narrow Transformations")

In privious code, the `where` statement specifies a narrow dependency, where only one partition contributes to at most one output partition.

With narrow tranformations, Spark will automatically perform an operation called `pipelining`, meaning that if specify multiple filters on DataFrames, they will all be performed `in-memory`.

### Wide Transformations  
A wide dependency style transformation will have input partitions contributing to many output partitions. It is also called as `Shuffle` whereby Spark will exchange partitions across the cluster.

When we perform a shuffle, Spark writes the results to `disk`.

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/spark/Wide_Transformations.PNG?raw=true "Narrow Transformations")

### Lazy Evaluation
Lazy evaluation means that Spark will wait unitl the very last moment to execute the graph of computation instructions. In Spark, instead of modifying the data immediately when we express some operation, Spark build up a plan of transformation that Spark would like to apply to source data.

By waiting untill the last minute to execute the code, Spark compiles this plan from raw DataFrame transformations to a streamlined physical plan that will run as efficiently as possible across cluster. This provides immense benefits, because Spark can optimize entire data flow from end to end.

An example if this is something call `predicate pushdown` on DataFrames. If we build a large Spark job but specify a filter at the end that only requires us to fetch one row from our source data, the most efficient way to execute this is to access the single record that we need. Spark will optimize this for us by pushing the filter down automatically.

## Actions