## Spark's Basic Architecture
A single computer or laptop works perfectly well for watching movies or working with spreadsheet software or working on small datasets, but there are some things that single computer is not powerful enough to perform. 

One particularly challenging area is data processing. Single machines do not have enough power and resources to perform computation on huge amount of information. A cluster or group of computers, pools the resources of many machines together, giving us the ability to use all the cumulative resources as if they were single computer. Now, a group of machines alone is not powerful, you need a framework to coordinate work across them. Spark does just that, managing and coordinating the execution of task on data across a cluster of computers.

The cluster of machines that Spark will use to execute tasks is managed by a cluster manager. Spark support many cluster managers -

- Spark Standalone cluster manager  
- YARN  
- MESOS  

We then submit Spark applications to these cluster manager, which will grant resources to our application so that we can complete our work.

### Spark Applications  
Spark Applications consist of **a driver process** and **a set of executor processes**. 

#### Driver Process  
The driver process which sits on a node in cluster and runs the main() function is responsible for 3 things-

- Maintaining information about the Spark Applications  
- Responding to a user's program or input  
- Analyzing, distributing, and scheduling work across the executors.

The driver process is absolutely essential. It is the heart of Spark applications and maintains all the relevant information during the lifetime of the application.

#### Executor Processes
The executors are responsible for actually carrying out the work that the driver assigns them. Executors are responsible for only 2 things- 

- Executing the code assigned to them by the driver  
- Reporting the state of computation on that executor back to the driver node.  

Below diagram demonstrate how the cluster manager controls the physical machines and allocates resources to Spark Applications. There could be multiple Spark applications running on a cluster at the same time.

![Alt text](https://github.com/vaibhavpatilai/Diagrams/blob/master/Spark_Basic_Architecture.PNG?raw=true "Spark Basic Architecture")

Spark, in addition to it's cluster mode, also has a **local mode**. The driver and executors are simply processes, which means that they can live on the same machine or different machines. In local mode, the driver and executors run(as thread) on single computer instead of a cluster.

Key points about Spark Applications-  

- Spark employs a cluster manager that keeps track of resources available  
- The driver prcess is responsible for executing the driver program's commands across the executors to complete given task.

Executors, for the most part will always be running Spark code. However, the driver can be driven from number of different language through Spark language APIs.